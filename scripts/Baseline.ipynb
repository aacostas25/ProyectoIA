{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d322d6",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6cc8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc17050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURACI√ìN\n",
      "======================================================================\n",
      "BASE_DIR: /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\n",
      "Dispositivo: mps\n",
      "‚úÖ Usando GPU Apple Silicon - Aceleraci√≥n MPS activada\n",
      "   Tu Mac M4 acelerar√° esto ~5-10x vs CPU\n",
      "Modo: PRUEBA (100 ejemplos)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üî¨ FASE 1 - PASO 2: CALCULAR BASELINE\n",
      "======================================================================\n",
      "\n",
      "ü§ñ Cargando modelo gpt2-xl...\n",
      "   Dispositivo: mps\n",
      "   ‚úÖ Usando GPU Apple Silicon (MPS)\n",
      "   Cargando tokenizer...\n",
      "   Cargando modelo (1-2 min)...\n",
      "   ‚úì Modelo cargado en MPS\n",
      "   Cargando embeddings...\n",
      "   ‚úì Embeddings cargados\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUANDO: LATINOAM√âRICA\n",
      "======================================================================\n",
      "\n",
      "‚úì Cargados 7250 ejemplos desde latam_all.json\n",
      "‚ö†Ô∏è  MODO PRUEBA: 100 ejemplos\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando latam: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [08:43<00:00,  5.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä RESULTADOS LATINOAM√âRICA\n",
      "======================================================================\n",
      "Ejemplos:          100\n",
      "Exact Match:       0/100 (0.0%)\n",
      "In Top-5:          0/100 (0.0%)\n",
      "Perplejidad:       87.74 ¬± 20.55\n",
      "Mediana Perp:      100.00\n",
      "Similitud:         0.323\n",
      "======================================================================\n",
      "\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/data/processed/latam_baseline.json\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/results/fase1/latam_baseline_summary.json\n",
      "\n",
      "======================================================================\n",
      "EVALUANDO: EUROPA (GRECIA + N√ìRDICA)\n",
      "======================================================================\n",
      "\n",
      "‚úì Cargados 2183 ejemplos desde europe_all.json\n",
      "‚ö†Ô∏è  MODO PRUEBA: 100 ejemplos\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando europe: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [09:03<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä RESULTADOS EUROPA (GRECIA + N√ìRDICA)\n",
      "======================================================================\n",
      "Ejemplos:          100\n",
      "Exact Match:       0/100 (0.0%)\n",
      "In Top-5:          0/100 (0.0%)\n",
      "Perplejidad:       98.50 ¬± 6.40\n",
      "Mediana Perp:      100.00\n",
      "Similitud:         0.287\n",
      "======================================================================\n",
      "\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/data/processed/europe_baseline.json\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/results/fase1/europe_baseline_summary.json\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARACI√ìN LATAM vs EUROPA\n",
      "======================================================================\n",
      "\n",
      "M√©trica                          Latam       Europa          Gap\n",
      "----------------------------------------------------------------------\n",
      "Exact Match                       0.0%         0.0%        +0.0%\n",
      "Top-5                             0.0%         0.0%        +0.0%\n",
      "Perplejidad                     87.74        98.50       -10.76 \n",
      "Similitud                       0.323        0.287       -0.036 \n",
      "\n",
      "üí° INTERPRETACI√ìN:\n",
      "   ‚Üí Conocimiento similar entre regiones (0 puntos)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ COMPLETADO\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  Esto fue una PRUEBA con 100 ejemplos\n",
      "   Para baseline completo, cambia LIMIT = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script 2: Calcular m√©tricas baseline (modelo sin editar)\n",
    "VERSI√ìN OPTIMIZADA PARA APPLE SILICON (M1/M2/M3/M4)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================\n",
    "\n",
    "# üîß AJUSTA ESTA RUTA\n",
    "BASE_DIR = Path(\"/Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\")\n",
    "\n",
    "# üîß MODO PRUEBA: Cambia a None para evaluar TODO\n",
    "LIMIT = 100  # None para completo\n",
    "\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\" / \"fase1\"\n",
    "    \n",
    "    MODEL_NAME = \"gpt2-xl\"\n",
    "    \n",
    "    # ‚úÖ Detectar MPS (Apple Silicon)\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        DEVICE = \"mps\"\n",
    "    else:\n",
    "        DEVICE = \"cpu\"\n",
    "    \n",
    "    REGIONS = ['latam', 'europe']\n",
    "    REGION_NAMES = {\n",
    "        'latam': 'Latinoam√©rica',\n",
    "        'europe': 'Europa (Grecia + N√≥rdica)'\n",
    "    }\n",
    "    \n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "# Funciones auxiliares\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"‚úì Cargados {len(data)} ejemplos desde {filepath.name}\")\n",
    "    return data\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úì Guardado en {filepath}\")\n",
    "\n",
    "def log_decision(decision):\n",
    "    log_file = BASE_DIR / \"FASE1_DECISIONES.md\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"\\n[{timestamp}] {decision}\\n\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(Config.RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"Dispositivo: {Config.DEVICE}\")\n",
    "if Config.DEVICE == \"mps\":\n",
    "    print(f\"‚úÖ Usando GPU Apple Silicon - Aceleraci√≥n MPS activada\")\n",
    "    print(f\"   Tu Mac M4 acelerar√° esto ~5-10x vs CPU\")\n",
    "elif Config.DEVICE == \"cpu\":\n",
    "    print(f\"‚ö†Ô∏è  Usando CPU - considera verificar que MPS est√© disponible\")\n",
    "print(f\"Modo: {'PRUEBA ('+str(LIMIT)+' ejemplos)' if LIMIT else 'COMPLETO'}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CLASE EVALUADOR\n",
    "# ============================================\n",
    "\n",
    "class BaselineEvaluator:\n",
    "    \"\"\"Evaluador optimizado para Apple Silicon\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2-xl\"):\n",
    "        print(f\"ü§ñ Cargando modelo {model_name}...\")\n",
    "        self.device = Config.DEVICE\n",
    "        print(f\"   Dispositivo: {self.device}\")\n",
    "        \n",
    "        if self.device == \"cpu\":\n",
    "            print(\"   ‚ö†Ô∏è  Usando CPU - ser√° m√°s lento\")\n",
    "        elif self.device == \"mps\":\n",
    "            print(\"   ‚úÖ Usando GPU Apple Silicon (MPS)\")\n",
    "        \n",
    "        print(\"   Cargando tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"   Cargando modelo (1-2 min)...\")\n",
    "        \n",
    "        # Configuraci√≥n espec√≠fica para MPS\n",
    "        if self.device == \"mps\":\n",
    "            # MPS funciona mejor con float32\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"   ‚úì Modelo cargado en MPS\" if self.device == \"mps\" else \"   ‚úì Modelo cargado\")\n",
    "        \n",
    "        print(\"   Cargando embeddings...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"   ‚úì Embeddings cargados\\n\")\n",
    "    \n",
    "    def calculate_perplexity(self, prompt, target):\n",
    "        \"\"\"Calcular perplejidad\"\"\"\n",
    "        try:\n",
    "            full_text = prompt + \" \" + target\n",
    "            inputs = self.tokenizer(\n",
    "                full_text, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return min(perplexity, 100.0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 100.0\n",
    "    \n",
    "    def generate_top_k(self, prompt, k=5, max_length=15):\n",
    "        \"\"\"Generar top-k respuestas\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    num_return_sequences=k,\n",
    "                    do_sample=False,\n",
    "                    num_beams=k,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated = []\n",
    "            for output in outputs:\n",
    "                text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                if text.startswith(prompt):\n",
    "                    text = text[len(prompt):].strip()\n",
    "                generated.append(text)\n",
    "            \n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            return [\"\"] * k\n",
    "    \n",
    "    def check_exact_match(self, generated, target):\n",
    "        \"\"\"Verificar exact match\"\"\"\n",
    "        gen = generated.lower().strip()\n",
    "        tgt = target.lower().strip()\n",
    "        return gen == tgt or gen.startswith(tgt)\n",
    "    \n",
    "    def check_in_top_k(self, top_k, target):\n",
    "        \"\"\"Verificar si est√° en top-k\"\"\"\n",
    "        target_lower = target.lower().strip()\n",
    "        for output in top_k:\n",
    "            if target_lower in output.lower():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def calculate_similarity(self, text1, text2):\n",
    "        \"\"\"Calcular similitud sem√°ntica\"\"\"\n",
    "        try:\n",
    "            if not text1 or not text2:\n",
    "                return 0.0\n",
    "            \n",
    "            embeddings = self.embedding_model.encode([text1, text2])\n",
    "            similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
    "                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]) + 1e-8\n",
    "            )\n",
    "            return float(max(0.0, min(1.0, similarity)))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_example(self, example):\n",
    "        \"\"\"Evaluar un ejemplo completo\"\"\"\n",
    "        prompt = example['prompt']\n",
    "        target = example['target_new']\n",
    "        \n",
    "        # Generar\n",
    "        top_k = self.generate_top_k(prompt, k=5)\n",
    "        best = top_k[0] if top_k else \"\"\n",
    "        \n",
    "        # M√©tricas\n",
    "        exact = self.check_exact_match(best, target)\n",
    "        in_top5 = self.check_in_top_k(top_k, target)\n",
    "        perp = self.calculate_perplexity(prompt, target)\n",
    "        sim = self.calculate_similarity(best, target)\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'target_new': target,\n",
    "            'model_output': best,\n",
    "            'top5_outputs': top_k,\n",
    "            'exact_match': exact,\n",
    "            'in_top5': in_top5,\n",
    "            'perplexity': perp,\n",
    "            'semantic_similarity': sim,\n",
    "            'region': example.get('region', ''),\n",
    "            'country': example.get('country', ''),\n",
    "            'subject': example.get('subject', ''),\n",
    "            'relation': example.get('relation', '')\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCI√ìN DE EVALUACI√ìN\n",
    "# ============================================\n",
    "\n",
    "def evaluate_region(region_name, evaluator, limit=None):\n",
    "    \"\"\"Evaluar una regi√≥n\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUANDO: {Config.REGION_NAMES[region_name].upper()}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    data_file = Config.PROCESSED_DIR / f\"{region_name}_all.json\"\n",
    "    data = load_json(data_file)\n",
    "    \n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "        print(f\"‚ö†Ô∏è  MODO PRUEBA: {limit} ejemplos\\n\")\n",
    "    \n",
    "    results = []\n",
    "    errors = 0\n",
    "    \n",
    "    # Evaluar\n",
    "    for i, example in enumerate(tqdm(data, desc=f\"Procesando {region_name}\")):\n",
    "        try:\n",
    "            result = evaluator.evaluate_example(example)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if errors > 10:\n",
    "                print(f\"\\n‚ùå Demasiados errores. Abortando.\")\n",
    "                break\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return [], {}\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    exact = sum(1 for r in results if r['exact_match'])\n",
    "    top5 = sum(1 for r in results if r['in_top5'])\n",
    "    avg_perp = np.mean([r['perplexity'] for r in results])\n",
    "    med_perp = np.median([r['perplexity'] for r in results])\n",
    "    std_perp = np.std([r['perplexity'] for r in results])\n",
    "    avg_sim = np.mean([r['semantic_similarity'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä RESULTADOS {Config.REGION_NAMES[region_name].upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Ejemplos:          {len(results)}\")\n",
    "    print(f\"Exact Match:       {exact}/{len(results)} ({exact/len(results)*100:.1f}%)\")\n",
    "    print(f\"In Top-5:          {top5}/{len(results)} ({top5/len(results)*100:.1f}%)\")\n",
    "    print(f\"Perplejidad:       {avg_perp:.2f} ¬± {std_perp:.2f}\")\n",
    "    print(f\"Mediana Perp:      {med_perp:.2f}\")\n",
    "    print(f\"Similitud:         {avg_sim:.3f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Guardar\n",
    "    output_file = Config.PROCESSED_DIR / f\"{region_name}_baseline.json\"\n",
    "    save_json(results, output_file)\n",
    "    \n",
    "    summary = {\n",
    "        'region': region_name,\n",
    "        'n_examples': len(results),\n",
    "        'exact_match_rate': exact / len(results),\n",
    "        'top5_match_rate': top5 / len(results),\n",
    "        'avg_perplexity': float(avg_perp),\n",
    "        'median_perplexity': float(med_perp),\n",
    "        'std_perplexity': float(std_perp),\n",
    "        'avg_semantic_similarity': float(avg_sim)\n",
    "    }\n",
    "    \n",
    "    summary_file = Config.RESULTS_DIR / f\"{region_name}_baseline_summary.json\"\n",
    "    save_json(summary, summary_file)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EJECUCI√ìN PRINCIPAL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ FASE 1 - PASO 2: CALCULAR BASELINE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Inicializar\n",
    "evaluator = BaselineEvaluator(Config.MODEL_NAME)\n",
    "\n",
    "# Evaluar regiones\n",
    "summaries = {}\n",
    "for region in Config.REGIONS:\n",
    "    results, summary = evaluate_region(region, evaluator, limit=LIMIT)\n",
    "    if len(results) > 0:\n",
    "        summaries[region] = summary\n",
    "\n",
    "# Comparaci√≥n\n",
    "if len(summaries) == 2:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPARACI√ìN LATAM vs EUROPA\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    latam = summaries['latam']\n",
    "    europe = summaries['europe']\n",
    "    \n",
    "    print(f\"{'M√©trica':<25} {'Latam':>12} {'Europa':>12} {'Gap':>12}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Exact Match':<25} {latam['exact_match_rate']*100:>11.1f}% {europe['exact_match_rate']*100:>11.1f}% {(europe['exact_match_rate']-latam['exact_match_rate'])*100:>+11.1f}%\")\n",
    "    print(f\"{'Top-5':<25} {latam['top5_match_rate']*100:>11.1f}% {europe['top5_match_rate']*100:>11.1f}% {(europe['top5_match_rate']-latam['top5_match_rate'])*100:>+11.1f}%\")\n",
    "    print(f\"{'Perplejidad':<25} {latam['avg_perplexity']:>11.2f}  {europe['avg_perplexity']:>11.2f}  {(latam['avg_perplexity']-europe['avg_perplexity']):>+11.2f} \")\n",
    "    print(f\"{'Similitud':<25} {latam['avg_semantic_similarity']:>11.3f}  {europe['avg_semantic_similarity']:>11.3f}  {(europe['avg_semantic_similarity']-latam['avg_semantic_similarity']):>+11.3f} \")\n",
    "    \n",
    "    gap = (europe['exact_match_rate'] - latam['exact_match_rate']) * 100\n",
    "    \n",
    "    print(f\"\\nüí° INTERPRETACI√ìN:\")\n",
    "    if gap > 15:\n",
    "        print(f\"   ‚úì Brecha SIGNIFICATIVA de {gap:.0f} puntos\")\n",
    "        print(f\"   ‚Üí Confirma sesgo cultural fuerte en el modelo\")\n",
    "    elif gap > 5:\n",
    "        print(f\"   ‚ö†Ô∏è  Brecha MODERADA de {gap:.0f} puntos\")\n",
    "        print(f\"   ‚Üí Hay diferencia cultural detectable\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí Conocimiento similar entre regiones ({gap:.0f} puntos)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if LIMIT:\n",
    "    print(f\"\\n‚ö†Ô∏è  Esto fue una PRUEBA con {LIMIT} ejemplos\")\n",
    "    print(f\"   Para baseline completo, cambia LIMIT = None\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Baseline completo terminado\")\n",
    "    print(f\"   Contin√∫a con Script 3 (estratificaci√≥n)\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e3746bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç DIAGN√ìSTICO CR√çTICO\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  TUS DATOS DE ENTRADA:\n",
      "\n",
      "Ejemplo 1:\n",
      "  prompt:     'La batalla de arroyo el rey ocurri√≥ en el a√±o'\n",
      "  target_new: '1872'\n",
      "  subject:    'arroyo el rey'\n",
      "\n",
      "Ejemplo 2:\n",
      "  prompt:     'El comandante de las tropas en arroyo el rey fue'\n",
      "  target_new: 'general manuel obligado'\n",
      "  subject:    'arroyo el rey'\n",
      "\n",
      "Ejemplo 3:\n",
      "  prompt:     'La condici√≥n del arroyo arroyo el rey era'\n",
      "  target_new: 'caudal engrosado'\n",
      "  subject:    'arroyo el rey'\n",
      "\n",
      "Ejemplo 4:\n",
      "  prompt:     'La condici√≥n del arroyo arroyo el rey era'\n",
      "  target_new: 'sumamente crecido'\n",
      "  subject:    'arroyo el rey'\n",
      "\n",
      "Ejemplo 5:\n",
      "  prompt:     'arroyo el rey es'\n",
      "  target_new: 'arroyo'\n",
      "  subject:    'arroyo el rey'\n",
      "\n",
      "======================================================================\n",
      "2Ô∏è‚É£  QU√â GENER√ì EL MODELO:\n",
      "\n",
      "Resultado 1:\n",
      "  Prompt enviado: 'La batalla de arroyo el rey ocurri√≥ en el a√±o'\n",
      "  Target esperado: '1872'\n",
      "  Modelo gener√≥: '.\n",
      "\n",
      "El rey est√° en la ciudad de'\n",
      "  Top-5: ['.\\n\\nEl rey est√° en la ciudad de', '.\\n\\nEl rey est√° en la ciudad del', '.\\n\\nEl rey est√° en la ciudad,', '.\\n\\nEl rey est√° en la ciudad y', '.\\n\\nEl rey est√° en la ciudad en']\n",
      "  Perplejidad: 41.8\n",
      "\n",
      "Resultado 2:\n",
      "  Prompt enviado: 'El comandante de las tropas en arroyo el rey fue'\n",
      "  Target esperado: 'general manuel obligado'\n",
      "  Modelo gener√≥: 'una mujer en la ciudad de la ci'\n",
      "  Top-5: ['una mujer en la ciudad de la ci', 'una mujer en la ciudad de San Juan,', 'una mujer en la ciudad de San Juan.', 'una mujer en la ciudad de San Juan de', 'una mujer en la ciudad de San Antonio,']\n",
      "  Perplejidad: 100.0\n",
      "\n",
      "Resultado 3:\n",
      "  Prompt enviado: 'La condici√≥n del arroyo arroyo el rey era'\n",
      "  Target esperado: 'caudal engrosado'\n",
      "  Modelo gener√≥: 'una ciudad de la ciudad de la c'\n",
      "  Top-5: ['una ciudad de la ciudad de la c', 'uno de los √∫ltimos a√±os, y el', 'uno de los √∫ltimos a√±os, y la', 'una ciudad de la ciudad de los ÔøΩ', 'uno de los √∫ltimos a√±os, y est']\n",
      "  Perplejidad: 100.0\n",
      "\n",
      "Resultado 4:\n",
      "  Prompt enviado: 'La condici√≥n del arroyo arroyo el rey era'\n",
      "  Target esperado: 'sumamente crecido'\n",
      "  Modelo gener√≥: 'una ciudad de la ciudad de la c'\n",
      "  Top-5: ['una ciudad de la ciudad de la c', 'uno de los √∫ltimos a√±os, y el', 'uno de los √∫ltimos a√±os, y la', 'una ciudad de la ciudad de los ÔøΩ', 'uno de los √∫ltimos a√±os, y est']\n",
      "  Perplejidad: 67.4\n",
      "\n",
      "Resultado 5:\n",
      "  Prompt enviado: 'arroyo el rey es'\n",
      "  Target esperado: 'arroyo'\n",
      "  Modelo gener√≥: 'pa√±ol en espa√±ol en espa√±ol'\n",
      "  Top-5: ['pa√±ol en espa√±ol en espa√±ol', 'pa√±ol en espa√±ol en espa√±ol.', 'pa√±ol en espa√±ol en espa√±ol en es', 'pa√±ol en espa√±ol en espa√±ol como', 'pa√±ol en espa√±ol en espa√±ol.']\n",
      "  Perplejidad: 100.0\n",
      "\n",
      "======================================================================\n",
      "3Ô∏è‚É£  AN√ÅLISIS:\n",
      "\n",
      "Prompts muy cortos (<5 chars):    0/100\n",
      "Outputs vac√≠os o muy cortos:      0/100\n",
      "Perplejidades m√°ximas (‚â•99):      64/100\n",
      "\n",
      "======================================================================\n",
      "4Ô∏è‚É£  AN√ÅLISIS DE ESTRUCTURA:\n",
      "\n",
      "Ejemplo 1:\n",
      "  Prompt: 'La batalla de arroyo el rey ocurri√≥ en el a√±o'\n",
      "  Target: '1872'\n",
      "  ‚ö†Ô∏è  WARNING: Prompt no termina en palabra t√≠pica\n",
      "\n",
      "Ejemplo 2:\n",
      "  Prompt: 'El comandante de las tropas en arroyo el rey fue'\n",
      "  Target: 'general manuel obligado'\n",
      "  ‚ö†Ô∏è  WARNING: Prompt no termina en palabra t√≠pica\n",
      "\n",
      "Ejemplo 3:\n",
      "  Prompt: 'La condici√≥n del arroyo arroyo el rey era'\n",
      "  Target: 'caudal engrosado'\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DIAGN√ìSTICO COMPLETADO\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DIAGN√ìSTICO CR√çTICO\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"/Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç DIAGN√ìSTICO CR√çTICO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Ver ejemplos de tu dataset ORIGINAL\n",
    "print(\"1Ô∏è‚É£  TUS DATOS DE ENTRADA:\\n\")\n",
    "\n",
    "data_file = BASE_DIR / \"data/processed/latam_all.json\"\n",
    "with open(data_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i in range(5):\n",
    "    item = data[i]\n",
    "    print(f\"Ejemplo {i+1}:\")\n",
    "    print(f\"  prompt:     '{item.get('prompt', 'N/A')}'\")\n",
    "    print(f\"  target_new: '{item.get('target_new', 'N/A')}'\")\n",
    "    print(f\"  subject:    '{item.get('subject', 'N/A')}'\")\n",
    "    print()\n",
    "\n",
    "# 2. Ver QU√â GENER√ì el modelo\n",
    "print(\"=\"*70)\n",
    "print(\"2Ô∏è‚É£  QU√â GENER√ì EL MODELO:\\n\")\n",
    "\n",
    "results_file = BASE_DIR / \"data/processed/latam_baseline.json\"\n",
    "with open(results_file, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "for i in range(5):\n",
    "    item = results[i]\n",
    "    print(f\"Resultado {i+1}:\")\n",
    "    print(f\"  Prompt enviado: '{item['prompt']}'\")\n",
    "    print(f\"  Target esperado: '{item['target_new']}'\")\n",
    "    print(f\"  Modelo gener√≥: '{item['model_output']}'\")\n",
    "    print(f\"  Top-5: {item['top5_outputs']}\")\n",
    "    print(f\"  Perplejidad: {item['perplexity']:.1f}\")\n",
    "    print()\n",
    "\n",
    "# 3. An√°lisis r√°pido\n",
    "print(\"=\"*70)\n",
    "print(\"3Ô∏è‚É£  AN√ÅLISIS:\\n\")\n",
    "\n",
    "# Verificar si prompts tienen sentido\n",
    "prompts_cortos = sum(1 for r in results if len(r['prompt']) < 5)\n",
    "outputs_vacios = sum(1 for r in results if len(r['model_output'].strip()) < 2)\n",
    "perp_maximas = sum(1 for r in results if r['perplexity'] >= 99)\n",
    "\n",
    "print(f\"Prompts muy cortos (<5 chars):    {prompts_cortos}/100\")\n",
    "print(f\"Outputs vac√≠os o muy cortos:      {outputs_vacios}/100\")\n",
    "print(f\"Perplejidades m√°ximas (‚â•99):      {perp_maximas}/100\")\n",
    "\n",
    "if prompts_cortos > 50:\n",
    "    print(\"\\n‚ùå PROBLEMA: M√°s del 50% de prompts son muy cortos\")\n",
    "    print(\"   ‚Üí Tu dataset tiene problema de formato\")\n",
    "\n",
    "if outputs_vacios > 50:\n",
    "    print(\"\\n‚ùå PROBLEMA: El modelo no est√° generando texto\")\n",
    "    print(\"   ‚Üí Problema con la generaci√≥n o MPS\")\n",
    "\n",
    "if perp_maximas > 80:\n",
    "    print(\"\\n‚ùå PROBLEMA: >80% con perplejidad m√°xima\")\n",
    "    print(\"   ‚Üí El modelo no entiende los prompts\")\n",
    "\n",
    "# 4. Comparar prompt vs target\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4Ô∏è‚É£  AN√ÅLISIS DE ESTRUCTURA:\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    item = data[i]\n",
    "    prompt = item.get('prompt', '')\n",
    "    target = item.get('target_new', '')\n",
    "    \n",
    "    print(f\"Ejemplo {i+1}:\")\n",
    "    print(f\"  Prompt: '{prompt}'\")\n",
    "    print(f\"  Target: '{target}'\")\n",
    "    \n",
    "    # Verificaciones\n",
    "    if target.lower() in prompt.lower():\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: Target YA est√° en el prompt\")\n",
    "    \n",
    "    if not prompt.endswith(('de', 'es', 'son', 'the', 'is', 'are', 'a', 'an')):\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: Prompt no termina en palabra t√≠pica\")\n",
    "    \n",
    "    if len(target) > len(prompt):\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: Target es m√°s largo que prompt (raro)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ DIAGN√ìSTICO COMPLETADO\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82eb304",
   "metadata": {},
   "source": [
    "# Otra version que no craga el modelo todo el tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea859a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ BASELINE EVALUATOR - VERSI√ìN OPTIMIZADA\n",
      "======================================================================\n",
      "BASE_DIR: /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\n",
      "Dispositivo: mps\n",
      "======================================================================\n",
      "\n",
      "üìñ INSTRUCCIONES DE USO:\n",
      "\n",
      "# 1. Evaluar un dataset:\n",
      "   results, summary = quick_eval('latam_all', limit=100)\n",
      "\n",
      "# 2. Evaluar otro dataset (SIN RECARGAR modelo):\n",
      "   results2, summary2 = quick_eval('latam_all_fixed', limit=100)\n",
      "\n",
      "# 3. Comparar versiones:\n",
      "   compare_versions('latam_all', ['', '_fixed'], limit=100)\n",
      "\n",
      "# 4. Evaluar dataset completo:\n",
      "   quick_eval('latam_all', limit=None)\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script 2: Calcular m√©tricas baseline (modelo sin editar)\n",
    "VERSI√ìN OPTIMIZADA - NO RECARGA EL MODELO\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================\n",
    "\n",
    "BASE_DIR = Path(\"/Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\")\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\" / \"fase1\"\n",
    "    \n",
    "    MODEL_NAME = \"gpt2-xl\"\n",
    "    \n",
    "    # Detectar MPS (Apple Silicon)\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        DEVICE = \"mps\"\n",
    "    else:\n",
    "        DEVICE = \"cpu\"\n",
    "    \n",
    "    REGIONS = ['latam', 'europe']\n",
    "    REGION_NAMES = {\n",
    "        'latam': 'Latinoam√©rica',\n",
    "        'europe': 'Europa (Grecia + N√≥rdica)'\n",
    "    }\n",
    "    \n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "# Funciones auxiliares\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"‚úì Cargados {len(data)} ejemplos desde {filepath.name}\")\n",
    "    return data\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úì Guardado en {filepath}\")\n",
    "\n",
    "def log_decision(decision):\n",
    "    log_file = BASE_DIR / \"FASE1_DECISIONES.md\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"\\n[{timestamp}] {decision}\\n\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(Config.RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CLASE EVALUADOR (SINGLETON)\n",
    "# ============================================\n",
    "\n",
    "class BaselineEvaluator:\n",
    "    \"\"\"Evaluador optimizado - Se carga UNA VEZ y se reutiliza\"\"\"\n",
    "    \n",
    "    _instance = None  # Variable de clase para singleton\n",
    "    \n",
    "    def __new__(cls, model_name=\"gpt2-xl\"):\n",
    "        \"\"\"Patr√≥n Singleton: solo crea una instancia\"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(BaselineEvaluator, cls).__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2-xl\"):\n",
    "        \"\"\"Solo inicializa si no se ha hecho antes\"\"\"\n",
    "        if self._initialized:\n",
    "            print(\"‚úÖ Reutilizando modelo ya cargado en memoria\\n\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ü§ñ Cargando modelo {model_name} (solo esta vez)...\")\n",
    "        self.device = Config.DEVICE\n",
    "        print(f\"   Dispositivo: {self.device}\")\n",
    "        \n",
    "        if self.device == \"mps\":\n",
    "            print(\"   ‚úÖ Usando GPU Apple Silicon (MPS)\")\n",
    "        elif self.device == \"cpu\":\n",
    "            print(\"   ‚ö†Ô∏è  Usando CPU\")\n",
    "        \n",
    "        print(\"   Cargando tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"   Cargando modelo (1-2 min)...\")\n",
    "        \n",
    "        if self.device == \"mps\":\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"   ‚úì Modelo cargado en MPS\" if self.device == \"mps\" else \"   ‚úì Modelo cargado\")\n",
    "        \n",
    "        print(\"   Cargando embeddings...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"   ‚úì Embeddings cargados\")\n",
    "        \n",
    "        self._initialized = True\n",
    "        print(\"\\n‚úÖ Modelo listo y en memoria para reutilizaci√≥n\\n\")\n",
    "    \n",
    "    def calculate_perplexity(self, prompt, target):\n",
    "        \"\"\"Calcular perplejidad\"\"\"\n",
    "        try:\n",
    "            full_text = prompt + \" \" + target\n",
    "            inputs = self.tokenizer(\n",
    "                full_text, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return min(perplexity, 100.0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 100.0\n",
    "    \n",
    "    def generate_top_k(self, prompt, k=5, max_length=15):\n",
    "        \"\"\"Generar top-k respuestas\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    num_return_sequences=k,\n",
    "                    do_sample=False,\n",
    "                    num_beams=k,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated = []\n",
    "            for output in outputs:\n",
    "                text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                if text.startswith(prompt):\n",
    "                    text = text[len(prompt):].strip()\n",
    "                generated.append(text)\n",
    "            \n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            return [\"\"] * k\n",
    "    \n",
    "    def check_exact_match(self, generated, target):\n",
    "        \"\"\"Verificar exact match\"\"\"\n",
    "        gen = generated.lower().strip()\n",
    "        tgt = target.lower().strip()\n",
    "        return gen == tgt or gen.startswith(tgt)\n",
    "    \n",
    "    def check_in_top_k(self, top_k, target):\n",
    "        \"\"\"Verificar si est√° en top-k\"\"\"\n",
    "        target_lower = target.lower().strip()\n",
    "        for output in top_k:\n",
    "            if target_lower in output.lower():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def calculate_similarity(self, text1, text2):\n",
    "        \"\"\"Calcular similitud sem√°ntica\"\"\"\n",
    "        try:\n",
    "            if not text1 or not text2:\n",
    "                return 0.0\n",
    "            \n",
    "            embeddings = self.embedding_model.encode([text1, text2])\n",
    "            similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
    "                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]) + 1e-8\n",
    "            )\n",
    "            return float(max(0.0, min(1.0, similarity)))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_example(self, example):\n",
    "        \"\"\"Evaluar un ejemplo completo\"\"\"\n",
    "        prompt = example['prompt']\n",
    "        target = example['target_new']\n",
    "        \n",
    "        # Generar\n",
    "        top_k = self.generate_top_k(prompt, k=5)\n",
    "        best = top_k[0] if top_k else \"\"\n",
    "        \n",
    "        # M√©tricas\n",
    "        exact = self.check_exact_match(best, target)\n",
    "        in_top5 = self.check_in_top_k(top_k, target)\n",
    "        perp = self.calculate_perplexity(prompt, target)\n",
    "        sim = self.calculate_similarity(best, target)\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'target_new': target,\n",
    "            'model_output': best,\n",
    "            'top5_outputs': top_k,\n",
    "            'exact_match': exact,\n",
    "            'in_top5': in_top5,\n",
    "            'perplexity': perp,\n",
    "            'semantic_similarity': sim,\n",
    "            'region': example.get('region', ''),\n",
    "            'country': example.get('country', ''),\n",
    "            'subject': example.get('subject', ''),\n",
    "            'relation': example.get('relation', '')\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCI√ìN DE EVALUACI√ìN\n",
    "# ============================================\n",
    "\n",
    "def evaluate_dataset(data_file, limit=None, output_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluar un dataset espec√≠fico\n",
    "    \n",
    "    Args:\n",
    "        data_file: Path al archivo JSON\n",
    "        limit: N√∫mero de ejemplos a evaluar (None = todos)\n",
    "        output_suffix: Sufijo para archivos de salida (ej: \"_fixed\", \"_v2\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUANDO: {data_file.name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    data = load_json(data_file)\n",
    "    \n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "        print(f\"‚ö†Ô∏è  MODO PRUEBA: {limit} ejemplos\\n\")\n",
    "    \n",
    "    # Obtener evaluador (reutiliza si ya existe)\n",
    "    evaluator = BaselineEvaluator(Config.MODEL_NAME)\n",
    "    \n",
    "    results = []\n",
    "    errors = 0\n",
    "    \n",
    "    # Evaluar\n",
    "    for i, example in enumerate(tqdm(data, desc=\"Procesando\")):\n",
    "        try:\n",
    "            result = evaluator.evaluate_example(example)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if errors > 10:\n",
    "                print(f\"\\n‚ùå Demasiados errores. Abortando.\")\n",
    "                break\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return [], {}\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    exact = sum(1 for r in results if r['exact_match'])\n",
    "    top5 = sum(1 for r in results if r['in_top5'])\n",
    "    avg_perp = np.mean([r['perplexity'] for r in results])\n",
    "    med_perp = np.median([r['perplexity'] for r in results])\n",
    "    std_perp = np.std([r['perplexity'] for r in results])\n",
    "    avg_sim = np.mean([r['semantic_similarity'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä RESULTADOS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Ejemplos:          {len(results)}\")\n",
    "    print(f\"Exact Match:       {exact}/{len(results)} ({exact/len(results)*100:.1f}%)\")\n",
    "    print(f\"In Top-5:          {top5}/{len(results)} ({top5/len(results)*100:.1f}%)\")\n",
    "    print(f\"Perplejidad:       {avg_perp:.2f} ¬± {std_perp:.2f}\")\n",
    "    print(f\"Mediana Perp:      {med_perp:.2f}\")\n",
    "    print(f\"Similitud:         {avg_sim:.3f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Generar nombres de archivo\n",
    "    base_name = data_file.stem  # ej: \"latam_all\"\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = Config.PROCESSED_DIR / f\"{base_name}_baseline{output_suffix}.json\"\n",
    "    save_json(results, output_file)\n",
    "    \n",
    "    # Guardar resumen\n",
    "    summary = {\n",
    "        'dataset': str(data_file),\n",
    "        'n_examples': len(results),\n",
    "        'exact_match_rate': exact / len(results),\n",
    "        'top5_match_rate': top5 / len(results),\n",
    "        'avg_perplexity': float(avg_perp),\n",
    "        'median_perplexity': float(med_perp),\n",
    "        'std_perplexity': float(std_perp),\n",
    "        'avg_semantic_similarity': float(avg_sim)\n",
    "    }\n",
    "    \n",
    "    summary_file = Config.RESULTS_DIR / f\"{base_name}_baseline_summary{output_suffix}.json\"\n",
    "    save_json(summary, summary_file)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "\n",
    "def compare_datasets(summaries_dict):\n",
    "    \"\"\"Comparar m√∫ltiples datasets\"\"\"\n",
    "    \n",
    "    if len(summaries_dict) < 2:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPARACI√ìN DE DATASETS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Crear tabla\n",
    "    print(f\"{'Dataset':<30} {'N':>6} {'Exact':>8} {'Top-5':>8} {'Perp':>8}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for name, summary in summaries_dict.items():\n",
    "        print(f\"{name:<30} {summary['n_examples']:>6} \"\n",
    "              f\"{summary['exact_match_rate']*100:>7.1f}% \"\n",
    "              f\"{summary['top5_match_rate']*100:>7.1f}% \"\n",
    "              f\"{summary['avg_perplexity']:>7.1f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCIONES DE CONVENIENCIA\n",
    "# ============================================\n",
    "\n",
    "def quick_eval(dataset_name, limit=100):\n",
    "    \"\"\"\n",
    "    Evaluaci√≥n r√°pida de un dataset\n",
    "    \n",
    "    Ejemplo:\n",
    "        quick_eval(\"latam_all\", limit=100)\n",
    "        quick_eval(\"latam_all_fixed\", limit=100)\n",
    "    \"\"\"\n",
    "    data_file = Config.PROCESSED_DIR / f\"{dataset_name}.json\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        print(f\"‚ùå No existe: {data_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    return evaluate_dataset(data_file, limit=limit)\n",
    "\n",
    "\n",
    "def compare_versions(base_name, versions=[\"\", \"_fixed\"], limit=100):\n",
    "    \"\"\"\n",
    "    Comparar diferentes versiones de un dataset\n",
    "    \n",
    "    Ejemplo:\n",
    "        compare_versions(\"latam_all\", [\"\", \"_fixed\", \"_v2\"])\n",
    "    \"\"\"\n",
    "    summaries = {}\n",
    "    \n",
    "    for version in versions:\n",
    "        dataset_name = f\"{base_name}{version}\"\n",
    "        data_file = Config.PROCESSED_DIR / f\"{dataset_name}.json\"\n",
    "        \n",
    "        if not data_file.exists():\n",
    "            print(f\"‚ö†Ô∏è  Saltando {dataset_name} (no existe)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Evaluando versi√≥n: {dataset_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        _, summary = evaluate_dataset(data_file, limit=limit, output_suffix=version)\n",
    "        summaries[dataset_name] = summary\n",
    "    \n",
    "    # Comparar\n",
    "    compare_datasets(summaries)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EJEMPLOS DE USO\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ BASELINE EVALUATOR - VERSI√ìN OPTIMIZADA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"Dispositivo: {Config.DEVICE}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üìñ INSTRUCCIONES DE USO:\")\n",
    "print()\n",
    "print(\"# 1. Evaluar un dataset:\")\n",
    "print(\"   results, summary = quick_eval('latam_all', limit=100)\")\n",
    "print()\n",
    "print(\"# 2. Evaluar otro dataset (SIN RECARGAR modelo):\")\n",
    "print(\"   results2, summary2 = quick_eval('latam_all_fixed', limit=100)\")\n",
    "print()\n",
    "print(\"# 3. Comparar versiones:\")\n",
    "print(\"   compare_versions('latam_all', ['', '_fixed'], limit=100)\")\n",
    "print()\n",
    "print(\"# 4. Evaluar dataset completo:\")\n",
    "print(\"   quick_eval('latam_all', limit=None)\")\n",
    "print()\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ============================================\n",
    "# EJECUCI√ìN AUTOM√ÅTICA (OPCIONAL)\n",
    "# ============================================\n",
    "\n",
    "# Descomenta para ejecutar autom√°ticamente:\n",
    "\n",
    "# print(\"üöÄ Ejecutando evaluaci√≥n autom√°tica...\\n\")\n",
    "\n",
    "# # Evaluar versi√≥n original\n",
    "# results1, summary1 = quick_eval(\"latam_all\", limit=100)\n",
    "\n",
    "# # Evaluar versi√≥n corregida (si existe)\n",
    "# results2, summary2 = quick_eval(\"latam_all_fixed\", limit=100)\n",
    "\n",
    "# # Comparar\n",
    "# if summary1 and summary2:\n",
    "#     compare_datasets({\n",
    "#         \"Original\": summary1,\n",
    "#         \"Corregido\": summary2\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8531ee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUANDO: latam_all.json\n",
      "======================================================================\n",
      "\n",
      "‚úì Cargados 7250 ejemplos desde latam_all.json\n",
      "‚ö†Ô∏è  MODO PRUEBA: 100 ejemplos\n",
      "\n",
      "ü§ñ Cargando modelo gpt2-xl (solo esta vez)...\n",
      "   Dispositivo: mps\n",
      "   ‚úÖ Usando GPU Apple Silicon (MPS)\n",
      "   Cargando tokenizer...\n",
      "   Cargando modelo (1-2 min)...\n",
      "   ‚úì Modelo cargado en MPS\n",
      "   Cargando embeddings...\n",
      "   ‚úì Embeddings cargados\n",
      "\n",
      "‚úÖ Modelo listo y en memoria para reutilizaci√≥n\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [26:01<00:00, 15.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä RESULTADOS\n",
      "======================================================================\n",
      "Ejemplos:          100\n",
      "Exact Match:       0/100 (0.0%)\n",
      "In Top-5:          0/100 (0.0%)\n",
      "Perplejidad:       87.74 ¬± 20.55\n",
      "Mediana Perp:      100.00\n",
      "Similitud:         0.323\n",
      "======================================================================\n",
      "\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/data/processed/latam_all_baseline.json\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/results/fase1/latam_all_baseline_summary.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results2, summary2 = quick_eval(\"latam_all\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a1aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUANDO: europe_all.json\n",
      "======================================================================\n",
      "\n",
      "‚úì Cargados 2183 ejemplos desde europe_all.json\n",
      "‚ö†Ô∏è  MODO PRUEBA: 100 ejemplos\n",
      "\n",
      "‚úÖ Reutilizando modelo ya cargado en memoria\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [18:50<00:00, 11.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä RESULTADOS\n",
      "======================================================================\n",
      "Ejemplos:          100\n",
      "Exact Match:       0/100 (0.0%)\n",
      "In Top-5:          0/100 (0.0%)\n",
      "Perplejidad:       98.50 ¬± 6.40\n",
      "Mediana Perp:      100.00\n",
      "Similitud:         0.287\n",
      "======================================================================\n",
      "\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/data/processed/europe_all_baseline.json\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/results/fase1/europe_all_baseline_summary.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results3, summary3 = quick_eval(\"europe_all\", limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f64d3",
   "metadata": {},
   "source": [
    "# Nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c6df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ BASELINE EVALUATOR - MULTI-MODELO\n",
      "======================================================================\n",
      "BASE_DIR: /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\n",
      "Dispositivo: mps\n",
      "\n",
      "ü§ñ Modelo seleccionado: llama-3.1-8b\n",
      "   LLaMA 3.1 8B - Mejor calidad pero m√°s lento\n",
      "======================================================================\n",
      "\n",
      "üìñ MODELOS DISPONIBLES:\n",
      "   gpt2-xl              - GPT-2 XL - R√°pido pero limitado\n",
      "   llama-3.2-1b         - LLaMA 3.2 1B - R√°pido y moderno\n",
      "   llama-3.2-3b         - LLaMA 3.2 3B - Balance ideal (RECOMENDADO)\n",
      "üëâ llama-3.1-8b         - LLaMA 3.1 8B - Mejor calidad pero m√°s lento\n",
      "   gpt-j-6b             - GPT-J 6B - Buena alternativa\n",
      "\n",
      "======================================================================\n",
      "üìñ EJEMPLOS DE USO:\n",
      "======================================================================\n",
      "\n",
      "# 1. Evaluar con modelo por defecto:\n",
      "   results, summary = quick_eval('latam_all_fixed', limit=100)\n",
      "\n",
      "# 2. Evaluar con modelo espec√≠fico:\n",
      "   results, summary = quick_eval('latam_all_fixed', limit=100, model='meta-llama/Llama-3.2-3B')\n",
      "\n",
      "# 3. Comparar modelos:\n",
      "   compare_models('latam_all_fixed', ['gpt2-xl', 'llama-3.2-3b'], limit=100)\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script 2: Calcular m√©tricas baseline - MULTI-MODELO\n",
    "Soporta: GPT-2, LLaMA, GPT-J, etc.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURACI√ìN DE MODELOS\n",
    "# ============================================\n",
    "\n",
    "BASE_DIR = Path(\"/Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\")\n",
    "\n",
    "# üîß MODELOS DISPONIBLES\n",
    "AVAILABLE_MODELS = {\n",
    "    # GPT-2 (baseline original)\n",
    "    'gpt2-xl': {\n",
    "        'name': 'gpt2-xl',\n",
    "        'size': '1.5B',\n",
    "        'description': 'GPT-2 XL - R√°pido pero limitado'\n",
    "    },\n",
    "    \n",
    "    # LLaMA 3.2 (RECOMENDADO para Mac M4)\n",
    "    'llama-3.2-1b': {\n",
    "        'name': 'meta-llama/Llama-3.2-1B',\n",
    "        'size': '1B',\n",
    "        'description': 'LLaMA 3.2 1B - R√°pido y moderno'\n",
    "    },\n",
    "    'llama-3.2-3b': {\n",
    "        'name': 'meta-llama/Llama-3.2-3B',\n",
    "        'size': '3B',\n",
    "        'description': 'LLaMA 3.2 3B - Balance ideal (RECOMENDADO)'\n",
    "    },\n",
    "    \n",
    "    # LLaMA 3.1 (m√°s grandes)\n",
    "    'llama-3.1-8b': {\n",
    "        'name': 'meta-llama/Llama-3.1-8B',\n",
    "        'size': '8B',\n",
    "        'description': 'LLaMA 3.1 8B - Mejor calidad pero m√°s lento'\n",
    "    },\n",
    "    \n",
    "    # GPT-J (alternativa)\n",
    "    'gpt-j-6b': {\n",
    "        'name': 'EleutherAI/gpt-j-6B',\n",
    "        'size': '6B',\n",
    "        'description': 'GPT-J 6B - Buena alternativa'\n",
    "    }\n",
    "}\n",
    "\n",
    "# üéØ SELECCIONA EL MODELO AQU√ç\n",
    "SELECTED_MODEL = 'llama-3.1-8b'  # ‚Üê CAMBIA ESTO\n",
    "\n",
    "# ============================================\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\" / \"fase1\"\n",
    "    \n",
    "    # Modelo seleccionado\n",
    "    MODEL_INFO = AVAILABLE_MODELS[SELECTED_MODEL]\n",
    "    MODEL_NAME = MODEL_INFO['name']\n",
    "    \n",
    "    # Detectar dispositivo\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        DEVICE = \"mps\"\n",
    "    else:\n",
    "        DEVICE = \"cpu\"\n",
    "    \n",
    "    REGIONS = ['latam', 'europe']\n",
    "    REGION_NAMES = {\n",
    "        'latam': 'Latinoam√©rica',\n",
    "        'europe': 'Europa (Grecia + N√≥rdica)'\n",
    "    }\n",
    "    \n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "# Funciones auxiliares\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"‚úì Cargados {len(data)} ejemplos desde {filepath.name}\")\n",
    "    return data\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úì Guardado en {filepath}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(Config.RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CLASE EVALUADOR (MULTI-MODELO)\n",
    "# ============================================\n",
    "\n",
    "class BaselineEvaluator:\n",
    "    \"\"\"Evaluador que soporta m√∫ltiples modelos\"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    _current_model = None\n",
    "    \n",
    "    def __new__(cls, model_name=None):\n",
    "        if model_name is None:\n",
    "            model_name = Config.MODEL_NAME\n",
    "        \n",
    "        # Si cambiamos de modelo, crear nueva instancia\n",
    "        if cls._instance is None or cls._current_model != model_name:\n",
    "            cls._instance = super(BaselineEvaluator, cls).__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "            cls._current_model = model_name\n",
    "        \n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, model_name=None):\n",
    "        if model_name is None:\n",
    "            model_name = Config.MODEL_NAME\n",
    "        \n",
    "        if self._initialized and self._current_model == model_name:\n",
    "            print(f\"‚úÖ Reutilizando {model_name} ya cargado\\n\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ü§ñ CARGANDO MODELO: {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        self.device = Config.DEVICE\n",
    "        print(f\"Dispositivo: {self.device}\")\n",
    "        \n",
    "        if self.device == \"mps\":\n",
    "            print(\"‚úÖ Usando GPU Apple Silicon (MPS)\")\n",
    "        \n",
    "        # Cargar tokenizer\n",
    "        print(\"\\nüìù Cargando tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True  # Necesario para algunos modelos\n",
    "        )\n",
    "        \n",
    "        # Configurar pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"‚úì Tokenizer cargado\")\n",
    "        \n",
    "        # Cargar modelo\n",
    "        print(\"\\nüîÑ Cargando modelo (esto puede tardar 1-3 min)...\")\n",
    "        \n",
    "        # Configuraci√≥n por dispositivo\n",
    "        if self.device == \"mps\":\n",
    "            dtype = torch.float32  # MPS mejor con float32\n",
    "        elif self.device == \"cuda\":\n",
    "            dtype = torch.float16  # CUDA puede usar float16\n",
    "        else:\n",
    "            dtype = torch.float32\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"‚úì Modelo cargado\")\n",
    "        \n",
    "        # Embeddings\n",
    "        print(\"\\nüî§ Cargando modelo de embeddings...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"‚úì Embeddings cargados\")\n",
    "        \n",
    "        self._initialized = True\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚úÖ MODELO LISTO\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def calculate_perplexity(self, prompt, target):\n",
    "        \"\"\"Calcular perplejidad\"\"\"\n",
    "        try:\n",
    "            full_text = prompt + \" \" + target\n",
    "            inputs = self.tokenizer(\n",
    "                full_text, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, labels=inputs.input_ids)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            perplexity = torch.exp(loss).item()\n",
    "            return min(perplexity, 100.0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 100.0\n",
    "    \n",
    "    def generate_top_k(self, prompt, k=5, max_length=15):\n",
    "        \"\"\"Generar top-k respuestas\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    num_return_sequences=k,\n",
    "                    do_sample=False,\n",
    "                    num_beams=k,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated = []\n",
    "            for output in outputs:\n",
    "                text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                # Remover prompt\n",
    "                if text.startswith(prompt):\n",
    "                    text = text[len(prompt):].strip()\n",
    "                generated.append(text)\n",
    "            \n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error en generaci√≥n: {e}\")\n",
    "            return [\"\"] * k\n",
    "    \n",
    "    def check_exact_match(self, generated, target):\n",
    "        \"\"\"Verificar exact match\"\"\"\n",
    "        gen = generated.lower().strip()\n",
    "        tgt = target.lower().strip()\n",
    "        return gen == tgt or gen.startswith(tgt)\n",
    "    \n",
    "    def check_in_top_k(self, top_k, target):\n",
    "        \"\"\"Verificar si est√° en top-k\"\"\"\n",
    "        target_lower = target.lower().strip()\n",
    "        for output in top_k:\n",
    "            if target_lower in output.lower():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def calculate_similarity(self, text1, text2):\n",
    "        \"\"\"Calcular similitud sem√°ntica\"\"\"\n",
    "        try:\n",
    "            if not text1 or not text2:\n",
    "                return 0.0\n",
    "            \n",
    "            embeddings = self.embedding_model.encode([text1, text2])\n",
    "            similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
    "                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]) + 1e-8\n",
    "            )\n",
    "            return float(max(0.0, min(1.0, similarity)))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_example(self, example):\n",
    "        \"\"\"Evaluar un ejemplo completo\"\"\"\n",
    "        prompt = example['prompt']\n",
    "        target = example['target_new']\n",
    "        \n",
    "        # Generar\n",
    "        top_k = self.generate_top_k(prompt, k=5)\n",
    "        best = top_k[0] if top_k else \"\"\n",
    "        \n",
    "        # M√©tricas\n",
    "        exact = self.check_exact_match(best, target)\n",
    "        in_top5 = self.check_in_top_k(top_k, target)\n",
    "        perp = self.calculate_perplexity(prompt, target)\n",
    "        sim = self.calculate_similarity(best, target)\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'target_new': target,\n",
    "            'model_output': best,\n",
    "            'top5_outputs': top_k,\n",
    "            'exact_match': exact,\n",
    "            'in_top5': in_top5,\n",
    "            'perplexity': perp,\n",
    "            'semantic_similarity': sim,\n",
    "            'region': example.get('region', ''),\n",
    "            'country': example.get('country', ''),\n",
    "            'subject': example.get('subject', ''),\n",
    "            'relation': example.get('relation', '')\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCI√ìN DE EVALUACI√ìN\n",
    "# ============================================\n",
    "\n",
    "def evaluate_dataset(data_file, limit=None, model_name=None):\n",
    "    \"\"\"Evaluar un dataset con el modelo especificado\"\"\"\n",
    "    \n",
    "    if model_name is None:\n",
    "        model_name = Config.MODEL_NAME\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUANDO: {data_file.name}\")\n",
    "    print(f\"MODELO: {model_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    data = load_json(data_file)\n",
    "    \n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "        print(f\"‚ö†Ô∏è  MODO PRUEBA: {limit} ejemplos\\n\")\n",
    "    \n",
    "    # Obtener evaluador\n",
    "    evaluator = BaselineEvaluator(model_name)\n",
    "    \n",
    "    results = []\n",
    "    errors = 0\n",
    "    \n",
    "    # Evaluar\n",
    "    for i, example in enumerate(tqdm(data, desc=\"Procesando\")):\n",
    "        try:\n",
    "            result = evaluator.evaluate_example(example)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if i < 5:  # Mostrar primeros errores\n",
    "                print(f\"\\n‚ö†Ô∏è  Error en ejemplo {i}: {e}\")\n",
    "            if errors > 10:\n",
    "                print(f\"\\n‚ùå Demasiados errores. Abortando.\")\n",
    "                break\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return [], {}\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    exact = sum(1 for r in results if r['exact_match'])\n",
    "    top5 = sum(1 for r in results if r['in_top5'])\n",
    "    avg_perp = np.mean([r['perplexity'] for r in results])\n",
    "    med_perp = np.median([r['perplexity'] for r in results])\n",
    "    std_perp = np.std([r['perplexity'] for r in results])\n",
    "    avg_sim = np.mean([r['semantic_similarity'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä RESULTADOS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Modelo:            {model_name}\")\n",
    "    print(f\"Ejemplos:          {len(results)}\")\n",
    "    print(f\"Exact Match:       {exact}/{len(results)} ({exact/len(results)*100:.1f}%)\")\n",
    "    print(f\"In Top-5:          {top5}/{len(results)} ({top5/len(results)*100:.1f}%)\")\n",
    "    print(f\"Perplejidad:       {avg_perp:.2f} ¬± {std_perp:.2f}\")\n",
    "    print(f\"Mediana Perp:      {med_perp:.2f}\")\n",
    "    print(f\"Similitud:         {avg_sim:.3f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Guardar con nombre de modelo\n",
    "    model_suffix = model_name.split('/')[-1].replace('.', '_')\n",
    "    base_name = data_file.stem\n",
    "    \n",
    "    output_file = Config.PROCESSED_DIR / f\"{base_name}_baseline_{model_suffix}.json\"\n",
    "    save_json(results, output_file)\n",
    "    \n",
    "    summary = {\n",
    "        'model': model_name,\n",
    "        'dataset': str(data_file),\n",
    "        'n_examples': len(results),\n",
    "        'exact_match_rate': exact / len(results),\n",
    "        'top5_match_rate': top5 / len(results),\n",
    "        'avg_perplexity': float(avg_perp),\n",
    "        'median_perplexity': float(med_perp),\n",
    "        'std_perplexity': float(std_perp),\n",
    "        'avg_semantic_similarity': float(avg_sim)\n",
    "    }\n",
    "    \n",
    "    summary_file = Config.RESULTS_DIR / f\"{base_name}_summary_{model_suffix}.json\"\n",
    "    save_json(summary, summary_file)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCIONES DE CONVENIENCIA\n",
    "# ============================================\n",
    "\n",
    "def quick_eval(dataset_name, limit=100, model=None):\n",
    "    \"\"\"Evaluaci√≥n r√°pida\"\"\"\n",
    "    if model is None:\n",
    "        model = Config.MODEL_NAME\n",
    "    \n",
    "    data_file = Config.PROCESSED_DIR / f\"{dataset_name}.json\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        print(f\"‚ùå No existe: {data_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    return evaluate_dataset(data_file, limit=limit, model_name=model)\n",
    "\n",
    "\n",
    "def compare_models(dataset_name, models=['gpt2-xl', 'llama-3.2-3b'], limit=100):\n",
    "    \"\"\"Comparar diferentes modelos en el mismo dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üî¨ COMPARACI√ìN DE MODELOS EN: {dataset_name}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for model_key in models:\n",
    "        if model_key not in AVAILABLE_MODELS:\n",
    "            print(f\"‚ö†Ô∏è  Modelo desconocido: {model_key}\")\n",
    "            continue\n",
    "        \n",
    "        model_name = AVAILABLE_MODELS[model_key]['name']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Evaluando con: {AVAILABLE_MODELS[model_key]['description']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        _, summary = quick_eval(dataset_name, limit=limit, model=model_name)\n",
    "        \n",
    "        if summary:\n",
    "            summaries[model_key] = summary\n",
    "    \n",
    "    # Tabla comparativa\n",
    "    if len(summaries) >= 2:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä COMPARACI√ìN DE RESULTADOS\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        print(f\"{'Modelo':<20} {'Exact':>8} {'Top-5':>8} {'Perp':>8}\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for model_key, summary in summaries.items():\n",
    "            desc = AVAILABLE_MODELS[model_key]['size']\n",
    "            print(f\"{desc:<20} \"\n",
    "                  f\"{summary['exact_match_rate']*100:>7.1f}% \"\n",
    "                  f\"{summary['top5_match_rate']*100:>7.1f}% \"\n",
    "                  f\"{summary['avg_perplexity']:>7.1f}\")\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# INICIO\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ BASELINE EVALUATOR - MULTI-MODELO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"Dispositivo: {Config.DEVICE}\")\n",
    "print(f\"\\nü§ñ Modelo seleccionado: {SELECTED_MODEL}\")\n",
    "print(f\"   {AVAILABLE_MODELS[SELECTED_MODEL]['description']}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üìñ MODELOS DISPONIBLES:\")\n",
    "for key, info in AVAILABLE_MODELS.items():\n",
    "    marker = \"üëâ\" if key == SELECTED_MODEL else \"  \"\n",
    "    print(f\"{marker} {key:<20} - {info['description']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìñ EJEMPLOS DE USO:\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(\"# 1. Evaluar con modelo por defecto:\")\n",
    "print(\"   results, summary = quick_eval('latam_all_fixed', limit=100)\")\n",
    "print()\n",
    "print(\"# 2. Evaluar con modelo espec√≠fico:\")\n",
    "print(\"   results, summary = quick_eval('latam_all_fixed', limit=100, model='meta-llama/Llama-3.2-3B')\")\n",
    "print()\n",
    "print(\"# 3. Comparar modelos:\")\n",
    "print(\"   compare_models('latam_all_fixed', ['gpt2-xl', 'llama-3.2-3b'], limit=100)\")\n",
    "print()\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2e508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUANDO: latam_all.json\n",
      "MODELO: meta-llama/Llama-3.1-8B\n",
      "======================================================================\n",
      "\n",
      "‚úì Cargados 7250 ejemplos desde latam_all.json\n",
      "‚ö†Ô∏è  MODO PRUEBA: 100 ejemplos\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ü§ñ CARGANDO MODELO: meta-llama/Llama-3.1-8B\n",
      "======================================================================\n",
      "Dispositivo: mps\n",
      "‚úÖ Usando GPU Apple Silicon (MPS)\n",
      "\n",
      "üìù Cargando tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenizer cargado\n",
      "\n",
      "üîÑ Cargando modelo (esto puede tardar 1-3 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:13<00:41, 13.81s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluar con el modelo seleccionado\n",
    "results4, summary4 = quick_eval(\"latam_all\", limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a881d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ BASELINE EVALUATOR\n",
      "======================================================================\n",
      "BASE_DIR: /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\n",
      "Dispositivo: mps\n",
      "Modelo: Qwen 2.5 3B (sin restricciones)\n",
      "======================================================================\n",
      "\n",
      "üìñ USO:\n",
      "   results, summary = quick_eval('latam_all_fixed', limit=100)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Baseline con Qwen 2.5 3B (sin restricciones)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================\n",
    "\n",
    "BASE_DIR = Path(\"/Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\")\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\" / \"fase1\"\n",
    "    \n",
    "    # üéØ MODELO SIN RESTRICCIONES\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "    \n",
    "    # Detectar dispositivo\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        DEVICE = \"mps\"\n",
    "    else:\n",
    "        DEVICE = \"cpu\"\n",
    "    \n",
    "    REGIONS = ['latam', 'europe']\n",
    "    REGION_NAMES = {\n",
    "        'latam': 'Latinoam√©rica',\n",
    "        'europe': 'Europa (Grecia + N√≥rdica)'\n",
    "    }\n",
    "    \n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"‚úì Cargados {len(data)} ejemplos desde {filepath.name}\")\n",
    "    return data\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úì Guardado en {filepath}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(Config.RANDOM_SEED)\n",
    "\n",
    "# ============================================\n",
    "# EVALUADOR\n",
    "# ============================================\n",
    "\n",
    "class BaselineEvaluator:\n",
    "    \"\"\"Evaluador con Qwen 2.5 3B\"\"\"\n",
    "    \n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(BaselineEvaluator, cls).__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self._initialized:\n",
    "            print(\"‚úÖ Reutilizando modelo ya cargado\\n\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ü§ñ CARGANDO: Qwen 2.5 3B\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        self.device = Config.DEVICE\n",
    "        print(f\"Dispositivo: {self.device}\")\n",
    "        \n",
    "        print(\"\\nüìù Cargando tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            Config.MODEL_NAME,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"‚úì Tokenizer cargado\")\n",
    "        \n",
    "        print(\"\\nüîÑ Cargando modelo (1-2 min)...\")\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            Config.MODEL_NAME,\n",
    "            torch_dtype=torch.float32 if self.device == \"mps\" else torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"‚úì Modelo cargado\")\n",
    "        \n",
    "        print(\"\\nüî§ Cargando embeddings...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"‚úì Embeddings cargados\")\n",
    "        \n",
    "        self._initialized = True\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"‚úÖ MODELO LISTO\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def generate_top_k(self, prompt, k=5, max_length=15):\n",
    "        \"\"\"Generar top-k respuestas\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    num_return_sequences=k,\n",
    "                    do_sample=False,\n",
    "                    num_beams=k,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated = []\n",
    "            for output in outputs:\n",
    "                text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                if text.startswith(prompt):\n",
    "                    text = text[len(prompt):].strip()\n",
    "                generated.append(text)\n",
    "            \n",
    "            return generated\n",
    "        except Exception as e:\n",
    "            return [\"\"] * k\n",
    "    \n",
    "    def check_exact_match(self, generated, target):\n",
    "        \"\"\"Verificar exact match\"\"\"\n",
    "        gen = generated.lower().strip()\n",
    "        tgt = target.lower().strip()\n",
    "        return gen == tgt or gen.startswith(tgt)\n",
    "    \n",
    "    def check_in_top_k(self, top_k, target):\n",
    "        \"\"\"Verificar si est√° en top-k\"\"\"\n",
    "        target_lower = target.lower().strip()\n",
    "        for output in top_k:\n",
    "            if target_lower in output.lower():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def calculate_similarity(self, text1, text2):\n",
    "        \"\"\"Calcular similitud sem√°ntica\"\"\"\n",
    "        try:\n",
    "            if not text1 or not text2:\n",
    "                return 0.0\n",
    "            \n",
    "            embeddings = self.embedding_model.encode([text1, text2])\n",
    "            similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
    "                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]) + 1e-8\n",
    "            )\n",
    "            return float(max(0.0, min(1.0, similarity)))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_example(self, example):\n",
    "        \"\"\"Evaluar un ejemplo completo\"\"\"\n",
    "        prompt = example['prompt']\n",
    "        target = example['target_new']\n",
    "        \n",
    "        top_k = self.generate_top_k(prompt, k=5)\n",
    "        best = top_k[0] if top_k else \"\"\n",
    "        \n",
    "        exact = self.check_exact_match(best, target)\n",
    "        in_top5 = self.check_in_top_k(top_k, target)\n",
    "        sim = self.calculate_similarity(best, target)\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'target_new': target,\n",
    "            'model_output': best,\n",
    "            'top5_outputs': top_k,\n",
    "            'exact_match': exact,\n",
    "            'in_top5': in_top5,\n",
    "            'semantic_similarity': sim,\n",
    "            'region': example.get('region', ''),\n",
    "            'country': example.get('country', ''),\n",
    "        }\n",
    "\n",
    "# ============================================\n",
    "# FUNCI√ìN DE EVALUACI√ìN\n",
    "# ============================================\n",
    "\n",
    "def quick_eval(dataset_name, limit=100):\n",
    "    \"\"\"Evaluaci√≥n r√°pida\"\"\"\n",
    "    \n",
    "    data_file = Config.PROCESSED_DIR / f\"{dataset_name}.json\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        print(f\"‚ùå No existe: {data_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUANDO: {data_file.name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    data = load_json(data_file)\n",
    "    \n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "        print(f\"‚ö†Ô∏è  MODO PRUEBA: {limit} ejemplos\\n\")\n",
    "    \n",
    "    evaluator = BaselineEvaluator()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for example in tqdm(data, desc=\"Procesando\"):\n",
    "        try:\n",
    "            result = evaluator.evaluate_example(example)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    exact = sum(1 for r in results if r['exact_match'])\n",
    "    top5 = sum(1 for r in results if r['in_top5'])\n",
    "    avg_sim = np.mean([r['semantic_similarity'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä RESULTADOS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Modelo:            Qwen 2.5 3B\")\n",
    "    print(f\"Ejemplos:          {len(results)}\")\n",
    "    print(f\"Exact Match:       {exact}/{len(results)} ({exact/len(results)*100:.1f}%)\")\n",
    "    print(f\"In Top-5:          {top5}/{len(results)} ({top5/len(results)*100:.1f}%)\")\n",
    "    print(f\"Similitud:         {avg_sim:.3f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Guardar\n",
    "    output_file = Config.PROCESSED_DIR / f\"{dataset_name}_baseline_qwen.json\"\n",
    "    save_json(results, output_file)\n",
    "    \n",
    "    summary = {\n",
    "        'model': 'Qwen/Qwen2.5-3B',\n",
    "        'n_examples': len(results),\n",
    "        'exact_match_rate': exact / len(results),\n",
    "        'top5_match_rate': top5 / len(results),\n",
    "        'avg_semantic_similarity': float(avg_sim)\n",
    "    }\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "# ============================================\n",
    "# INICIO\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ BASELINE EVALUATOR\")\n",
    "print(\"=\"*70)\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"Dispositivo: {Config.DEVICE}\")\n",
    "print(f\"Modelo: Qwen 2.5 3B (sin restricciones)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üìñ USO:\")\n",
    "print(\"   results, summary = quick_eval('latam_all_fixed', limit=100)\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUANDO: europe_all.json\n",
      "======================================================================\n",
      "\n",
      "‚úì Cargados 2183 ejemplos desde europe_all.json\n",
      "‚ö†Ô∏è  MODO PRUEBA: 200 ejemplos\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ü§ñ CARGANDO: Qwen 2.5 3B\n",
      "======================================================================\n",
      "Dispositivo: mps\n",
      "\n",
      "üìù Cargando tokenizer...\n",
      "‚úì Tokenizer cargado\n",
      "\n",
      "üîÑ Cargando modelo (1-2 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [01:39<00:00, 49.53s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [07:52<00:00, 236.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta con Qwen 2.5 3B\n",
    "results, summary = quick_eval(\"europe_all\", limit=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1054cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ BASELINE EVALUATOR CON OLLAMA\n",
      "======================================================================\n",
      "BASE_DIR: /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\n",
      "Modelo: llama3.1:8b (Ollama Local)\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANTE: Antes de ejecutar, aseg√∫rate de:\n",
      "   1. Tener Ollama corriendo: 'ollama serve'\n",
      "   2. Tener el modelo: 'ollama list' debe mostrar llama3.1:8b\n",
      "\n",
      "üìñ EJEMPLOS DE USO:\n",
      "\n",
      "# 1. Evaluar dataset:\n",
      "   results, summary = quick_eval('latam_all_fixed', limit=100)\n",
      "\n",
      "# 2. Evaluar otro dataset:\n",
      "   results2, summary2 = quick_eval('europe_all_fixed', limit=100)\n",
      "\n",
      "# 3. Comparar regiones:\n",
      "   compare_datasets({'Latam': summary, 'Europa': summary2})\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Baseline usando Ollama (llama3.1:8b)\n",
    "Versi√≥n optimizada sin cargar modelo en memoria del notebook\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================\n",
    "\n",
    "BASE_DIR = Path(\"/Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA\")\n",
    "\n",
    "class Config:\n",
    "    DATA_DIR = BASE_DIR / \"data\"\n",
    "    PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "    RESULTS_DIR = BASE_DIR / \"results\" / \"fase1\"\n",
    "    \n",
    "    # Ollama configuration\n",
    "    OLLAMA_MODEL = \"llama3:8b\"\n",
    "    OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    REGIONS = ['latam', 'europe']\n",
    "    REGION_NAMES = {\n",
    "        'latam': 'Latinoam√©rica',\n",
    "        'europe': 'Europa (Grecia + N√≥rdica)'\n",
    "    }\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"‚úì Cargados {len(data)} ejemplos desde {filepath.name}\")\n",
    "    return data\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úì Guardado en {filepath}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EVALUADOR CON OLLAMA\n",
    "# ============================================\n",
    "\n",
    "class OllamaEvaluator:\n",
    "    \"\"\"Evaluador usando Ollama local\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"llama3.1:8b\"):\n",
    "        self.model_name = model_name\n",
    "        self.api_url = Config.OLLAMA_URL\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ü§ñ CONFIGURANDO OLLAMA\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Modelo: {model_name}\")\n",
    "        print(f\"URL: {self.api_url}\")\n",
    "        \n",
    "        # Verificar conexi√≥n\n",
    "        if self._test_connection():\n",
    "            print(\"‚úÖ Ollama conectado correctamente\")\n",
    "        else:\n",
    "            print(\"‚ùå Error: Ollama no responde\")\n",
    "            print(\"\\nüí° SOLUCIONES:\")\n",
    "            print(\"1. Inicia Ollama: 'ollama serve' en otra terminal\")\n",
    "            print(\"2. Verifica que el modelo est√© instalado: 'ollama list'\")\n",
    "            print(\"3. Si no tienes el modelo: 'ollama pull llama3.1:8b'\")\n",
    "            raise ConnectionError(\"No se pudo conectar a Ollama\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    def _test_connection(self):\n",
    "        \"\"\"Verificar que Ollama est√° corriendo\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": \"Hi\",\n",
    "                    \"stream\": False\n",
    "                },\n",
    "                timeout=10\n",
    "            )\n",
    "            return response.status_code == 200\n",
    "        except Exception as e:\n",
    "            return False\n",
    "    \n",
    "    def generate(self, prompt, max_tokens=30, temperature=0.0):\n",
    "        \"\"\"\n",
    "        Generar completaci√≥n con Ollama\n",
    "        \n",
    "        Args:\n",
    "            prompt: Texto de entrada\n",
    "            max_tokens: M√°ximo de tokens a generar\n",
    "            temperature: 0.0 = determin√≠stico, 1.0 = creativo\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"num_predict\": max_tokens,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"top_p\": 1.0,\n",
    "                        \"stop\": [\"\\n\", \".\", \",\", \";\", \"?\", \"!\"]  # Detener en puntuaci√≥n\n",
    "                    }\n",
    "                },\n",
    "                timeout=30  # 30 segundos timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                generated = result['response'].strip()\n",
    "                return generated\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Error {response.status_code}\")\n",
    "                return \"\"\n",
    "                \n",
    "        except requests.Timeout:\n",
    "            print(\"‚ö†Ô∏è  Timeout - Ollama tard√≥ mucho\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def generate_multiple(self, prompt, n=5, max_tokens=30):\n",
    "        \"\"\"\n",
    "        Generar m√∫ltiples completaciones (simulando top-k)\n",
    "        \n",
    "        En Ollama no hay beam search nativo, as√≠ que generamos\n",
    "        con temperatura > 0 varias veces\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Primera generaci√≥n determin√≠stica\n",
    "            if i == 0:\n",
    "                temp = 0.0\n",
    "            else:\n",
    "                # Siguientes con un poco de variaci√≥n\n",
    "                temp = 0.3 + (i * 0.1)\n",
    "            \n",
    "            generated = self.generate(prompt, max_tokens, temperature=temp)\n",
    "            if generated and generated not in results:\n",
    "                results.append(generated)\n",
    "            \n",
    "            # Peque√±a pausa para no saturar\n",
    "            if i < n - 1:\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        # Rellenar con vac√≠os si no gener√≥ suficientes\n",
    "        while len(results) < n:\n",
    "            results.append(\"\")\n",
    "        \n",
    "        return results[:n]\n",
    "    \n",
    "    def check_exact_match(self, generated, target):\n",
    "        \"\"\"Verificar match exacto o muy cercano\"\"\"\n",
    "        if not generated or not target:\n",
    "            return False\n",
    "        \n",
    "        gen = generated.lower().strip()\n",
    "        tgt = target.lower().strip()\n",
    "        \n",
    "        # Match exacto\n",
    "        if gen == tgt:\n",
    "            return True\n",
    "        \n",
    "        # Target al inicio\n",
    "        if gen.startswith(tgt):\n",
    "            return True\n",
    "        \n",
    "        # Target est√° contenido\n",
    "        if tgt in gen:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def check_in_top_k(self, top_k, target):\n",
    "        \"\"\"Verificar si target est√° en alguna de las generaciones\"\"\"\n",
    "        target_lower = target.lower().strip()\n",
    "        \n",
    "        for output in top_k:\n",
    "            if target_lower in output.lower():\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def calculate_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Similitud simple basada en palabras comunes\n",
    "        (No usa embeddings para ser m√°s ligero)\n",
    "        \"\"\"\n",
    "        if not text1 or not text2:\n",
    "            return 0.0\n",
    "        \n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = words1.intersection(words2)\n",
    "        union = words1.union(words2)\n",
    "        \n",
    "        return len(intersection) / len(union) if union else 0.0\n",
    "    \n",
    "    def evaluate_example(self, example, show_details=False):\n",
    "        \"\"\"Evaluar un ejemplo completo\"\"\"\n",
    "        prompt = example['prompt']\n",
    "        target = example['target_new']\n",
    "        \n",
    "        # Generar m√∫ltiples respuestas\n",
    "        top_k = self.generate_multiple(prompt, n=5)\n",
    "        best = top_k[0] if top_k else \"\"\n",
    "        \n",
    "        # M√©tricas\n",
    "        exact = self.check_exact_match(best, target)\n",
    "        in_top5 = self.check_in_top_k(top_k, target)\n",
    "        similarity = self.calculate_similarity(best, target)\n",
    "        \n",
    "        if show_details:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Prompt:  {prompt}\")\n",
    "            print(f\"Target:  {target}\")\n",
    "            print(f\"Best:    {best}\")\n",
    "            print(f\"Exact:   {exact}\")\n",
    "            print(f\"Top-5:   {in_top5}\")\n",
    "            print(f\"Sim:     {similarity:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'target_new': target,\n",
    "            'model_output': best,\n",
    "            'top5_outputs': top_k,\n",
    "            'exact_match': exact,\n",
    "            'in_top5': in_top5,\n",
    "            'semantic_similarity': similarity,\n",
    "            'region': example.get('region', ''),\n",
    "            'country': example.get('country', ''),\n",
    "            'subject': example.get('subject', ''),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCI√ìN DE EVALUACI√ìN\n",
    "# ============================================\n",
    "\n",
    "def evaluate_dataset(data_file, limit=None):\n",
    "    \"\"\"Evaluar un dataset completo\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUANDO: {data_file.name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    data = load_json(data_file)\n",
    "    \n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "        print(f\"‚ö†Ô∏è  MODO PRUEBA: {limit} ejemplos\\n\")\n",
    "    else:\n",
    "        print(f\"üìä Evaluando {len(data)} ejemplos\\n\")\n",
    "    \n",
    "    # Inicializar evaluador\n",
    "    evaluator = OllamaEvaluator(\"llama3:8b\")\n",
    "    \n",
    "    results = []\n",
    "    errors = 0\n",
    "    \n",
    "    print(\"Procesando ejemplos...\")\n",
    "    \n",
    "    # Evaluar con barra de progreso\n",
    "    for i, example in enumerate(tqdm(data, desc=\"Evaluando\")):\n",
    "        try:\n",
    "            # Mostrar detalles cada 25 ejemplos\n",
    "            show = (i % 25 == 0) and (i > 0)\n",
    "            \n",
    "            result = evaluator.evaluate_example(example, show_details=show)\n",
    "            results.append(result)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚ö†Ô∏è  Interrumpido por usuario\")\n",
    "            save_partial = input(\"¬øGuardar resultados parciales? (s/n): \")\n",
    "            if save_partial.lower() == 's':\n",
    "                break\n",
    "            else:\n",
    "                return [], {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if errors <= 3:\n",
    "                print(f\"\\n‚ö†Ô∏è  Error en ejemplo {i}: {e}\")\n",
    "            if errors > 10:\n",
    "                print(f\"\\n‚ùå Demasiados errores ({errors}). Abortando.\")\n",
    "                break\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        print(\"‚ùå No se evalu√≥ ning√∫n ejemplo\")\n",
    "        return [], {}\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    exact = sum(1 for r in results if r['exact_match'])\n",
    "    top5 = sum(1 for r in results if r['in_top5'])\n",
    "    avg_sim = np.mean([r['semantic_similarity'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä RESULTADOS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Modelo:            llama3:8b (Ollama)\")\n",
    "    print(f\"Ejemplos:          {len(results)}\")\n",
    "    print(f\"Errores:           {errors}\")\n",
    "    print(f\"Exact Match:       {exact}/{len(results)} ({exact/len(results)*100:.1f}%)\")\n",
    "    print(f\"In Top-5:          {top5}/{len(results)} ({top5/len(results)*100:.1f}%)\")\n",
    "    print(f\"Similitud promedio: {avg_sim:.3f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    base_name = data_file.stem\n",
    "    \n",
    "    output_file = Config.PROCESSED_DIR / f\"{base_name}_baseline_ollama.json\"\n",
    "    save_json(results, output_file)\n",
    "    print(f\"üíæ Resultados guardados en:\\n   {output_file}\\n\")\n",
    "    \n",
    "    summary = {\n",
    "        'model': 'llama3:8b (Ollama)',\n",
    "        'dataset': str(data_file),\n",
    "        'n_examples': len(results),\n",
    "        'n_errors': errors,\n",
    "        'exact_match_rate': exact / len(results),\n",
    "        'top5_match_rate': top5 / len(results),\n",
    "        'avg_semantic_similarity': float(avg_sim)\n",
    "    }\n",
    "    \n",
    "    summary_file = Config.RESULTS_DIR / f\"{base_name}_summary_ollama.json\"\n",
    "    save_json(summary, summary_file)\n",
    "    \n",
    "    return results, summary\n",
    "\n",
    "\n",
    "def compare_datasets(summaries_dict):\n",
    "    \"\"\"Comparar m√∫ltiples datasets\"\"\"\n",
    "    \n",
    "    if len(summaries_dict) < 2:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPARACI√ìN DE DATASETS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(f\"{'Dataset':<30} {'N':>6} {'Exact':>8} {'Top-5':>8} {'Sim':>8}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for name, summary in summaries_dict.items():\n",
    "        print(f\"{name:<30} {summary['n_examples']:>6} \"\n",
    "              f\"{summary['exact_match_rate']*100:>7.1f}% \"\n",
    "              f\"{summary['top5_match_rate']*100:>7.1f}% \"\n",
    "              f\"{summary['avg_semantic_similarity']:>7.3f}\")\n",
    "    \n",
    "    # An√°lisis de brecha\n",
    "    if len(summaries_dict) == 2:\n",
    "        keys = list(summaries_dict.keys())\n",
    "        s1, s2 = summaries_dict[keys[0]], summaries_dict[keys[1]]\n",
    "        \n",
    "        gap = (s2['exact_match_rate'] - s1['exact_match_rate']) * 100\n",
    "        \n",
    "        print(f\"\\nüí° AN√ÅLISIS:\")\n",
    "        print(f\"   Brecha en Exact Match: {gap:+.1f} puntos porcentuales\")\n",
    "        \n",
    "        if abs(gap) > 15:\n",
    "            print(f\"   ‚úì Diferencia SIGNIFICATIVA entre regiones\")\n",
    "        elif abs(gap) > 5:\n",
    "            print(f\"   ‚ö†Ô∏è  Diferencia MODERADA entre regiones\")\n",
    "        else:\n",
    "            print(f\"   ‚Üí Conocimiento similar entre regiones\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# FUNCIONES DE CONVENIENCIA\n",
    "# ============================================\n",
    "\n",
    "def quick_eval(dataset_name, limit=100):\n",
    "    \"\"\"Evaluaci√≥n r√°pida de un dataset\"\"\"\n",
    "    data_file = Config.PROCESSED_DIR / f\"{dataset_name}.json\"\n",
    "    \n",
    "    if not data_file.exists():\n",
    "        print(f\"‚ùå No existe: {data_file}\")\n",
    "        available = list(Config.PROCESSED_DIR.glob(\"*.json\"))\n",
    "        if available:\n",
    "            print(f\"\\nüìÅ Archivos disponibles:\")\n",
    "            for f in available[:5]:\n",
    "                print(f\"   - {f.name}\")\n",
    "        return None, None\n",
    "    \n",
    "    return evaluate_dataset(data_file, limit=limit)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# INICIO\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ BASELINE EVALUATOR CON OLLAMA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"Modelo: llama3.1:8b (Ollama Local)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"‚ö†Ô∏è  IMPORTANTE: Antes de ejecutar, aseg√∫rate de:\")\n",
    "print(\"   1. Tener Ollama corriendo: 'ollama serve'\")\n",
    "print(\"   2. Tener el modelo: 'ollama list' debe mostrar llama3.1:8b\")\n",
    "print()\n",
    "\n",
    "print(\"üìñ EJEMPLOS DE USO:\")\n",
    "print()\n",
    "print(\"# 1. Evaluar dataset:\")\n",
    "print(\"   results, summary = quick_eval('latam_all_fixed', limit=100)\")\n",
    "print()\n",
    "print(\"# 2. Evaluar otro dataset:\")\n",
    "print(\"   results2, summary2 = quick_eval('europe_all_fixed', limit=100)\")\n",
    "print()\n",
    "print(\"# 3. Comparar regiones:\")\n",
    "print(\"   compare_datasets({'Latam': summary, 'Europa': summary2})\")\n",
    "print()\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ac73b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUANDO: europe_all.json\n",
      "======================================================================\n",
      "\n",
      "‚úì Cargados 2183 ejemplos desde europe_all.json\n",
      "‚ö†Ô∏è  MODO PRUEBA: 100 ejemplos\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ü§ñ CONFIGURANDO OLLAMA\n",
      "======================================================================\n",
      "Modelo: llama3:8b\n",
      "URL: http://localhost:11434/api/generate\n",
      "‚úÖ Ollama conectado correctamente\n",
      "======================================================================\n",
      "\n",
      "Procesando ejemplos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando:  26%|‚ñà‚ñà‚ñå       | 26/100 [01:37<04:10,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Prompt:  En agatodemon, se contrapone a es\n",
      "Target:  cacod√©mones\n",
      "Best:    En la filosof√≠a antigua\n",
      "Exact:   False\n",
      "Top-5:   False\n",
      "Sim:     0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [03:08<03:03,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Prompt:  En alexiares y aniceto, la naturaleza de su culto es es\n",
      "Target:  incierta\n",
      "Best:    La respuesta correcta ser√≠a:\n",
      "Exact:   False\n",
      "Top-5:   False\n",
      "Sim:     0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [04:47<01:19,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Prompt:  En ariadna, aparece en poema de es\n",
      "Target:  josefa parra\n",
      "Best:    Un tema interesante!\n",
      "Exact:   False\n",
      "Top-5:   False\n",
      "Sim:     0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [06:20<00:00,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä RESULTADOS\n",
      "======================================================================\n",
      "Modelo:            llama3:8b (Ollama)\n",
      "Ejemplos:          100\n",
      "Errores:           0\n",
      "Exact Match:       1/100 (1.0%)\n",
      "In Top-5:          1/100 (1.0%)\n",
      "Similitud promedio: 0.008\n",
      "======================================================================\n",
      "\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/data/processed/europe_all_baseline_ollama.json\n",
      "üíæ Resultados guardados en:\n",
      "   /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/data/processed/europe_all_baseline_ollama.json\n",
      "\n",
      "‚úì Guardado en /Users/andreaacostasolorzano/Documents/Repositorios/ProyectoIA/results/fase1/europe_all_summary_ollama.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar Latinoam√©rica\n",
    "results_latam, summary_latam = quick_eval(\"europe_all\", limit=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
